{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1236c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f003bfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = resnet18()\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d91ab",
   "metadata": {},
   "source": [
    "## 模型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5468e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba59dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    jit_model = torch.jit.trace(model, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e71155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  original_name=ResNet\n",
       "  (conv1): Conv2d(original_name=Conv2d)\n",
       "  (bn1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "  (relu): ReLU(original_name=ReLU)\n",
       "  (maxpool): MaxPool2d(original_name=MaxPool2d)\n",
       "  (layer1): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): BasicBlock(\n",
       "      original_name=BasicBlock\n",
       "      (conv1): Conv2d(original_name=Conv2d)\n",
       "      (bn1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (conv2): Conv2d(original_name=Conv2d)\n",
       "      (bn2): BatchNorm2d(original_name=BatchNorm2d)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      original_name=BasicBlock\n",
       "      (conv1): Conv2d(original_name=Conv2d)\n",
       "      (bn1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (conv2): Conv2d(original_name=Conv2d)\n",
       "      (bn2): BatchNorm2d(original_name=BatchNorm2d)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): BasicBlock(\n",
       "      original_name=BasicBlock\n",
       "      (conv1): Conv2d(original_name=Conv2d)\n",
       "      (bn1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (conv2): Conv2d(original_name=Conv2d)\n",
       "      (bn2): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      (downsample): Sequential(\n",
       "        original_name=Sequential\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      original_name=BasicBlock\n",
       "      (conv1): Conv2d(original_name=Conv2d)\n",
       "      (bn1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (conv2): Conv2d(original_name=Conv2d)\n",
       "      (bn2): BatchNorm2d(original_name=BatchNorm2d)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): BasicBlock(\n",
       "      original_name=BasicBlock\n",
       "      (conv1): Conv2d(original_name=Conv2d)\n",
       "      (bn1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (conv2): Conv2d(original_name=Conv2d)\n",
       "      (bn2): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      (downsample): Sequential(\n",
       "        original_name=Sequential\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      original_name=BasicBlock\n",
       "      (conv1): Conv2d(original_name=Conv2d)\n",
       "      (bn1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (conv2): Conv2d(original_name=Conv2d)\n",
       "      (bn2): BatchNorm2d(original_name=BatchNorm2d)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): BasicBlock(\n",
       "      original_name=BasicBlock\n",
       "      (conv1): Conv2d(original_name=Conv2d)\n",
       "      (bn1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (conv2): Conv2d(original_name=Conv2d)\n",
       "      (bn2): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      (downsample): Sequential(\n",
       "        original_name=Sequential\n",
       "        (0): Conv2d(original_name=Conv2d)\n",
       "        (1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      original_name=BasicBlock\n",
       "      (conv1): Conv2d(original_name=Conv2d)\n",
       "      (bn1): BatchNorm2d(original_name=BatchNorm2d)\n",
       "      (relu): ReLU(original_name=ReLU)\n",
       "      (conv2): Conv2d(original_name=Conv2d)\n",
       "      (bn2): BatchNorm2d(original_name=BatchNorm2d)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(original_name=AdaptiveAvgPool2d)\n",
       "  (fc): Linear(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e79a4e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  original_name=Sequential\n",
      "  (0): BasicBlock(\n",
      "    original_name=BasicBlock\n",
      "    (conv1): Conv2d(original_name=Conv2d)\n",
      "    (bn1): BatchNorm2d(original_name=BatchNorm2d)\n",
      "    (relu): ReLU(original_name=ReLU)\n",
      "    (conv2): Conv2d(original_name=Conv2d)\n",
      "    (bn2): BatchNorm2d(original_name=BatchNorm2d)\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    original_name=BasicBlock\n",
      "    (conv1): Conv2d(original_name=Conv2d)\n",
      "    (bn1): BatchNorm2d(original_name=BatchNorm2d)\n",
      "    (relu): ReLU(original_name=ReLU)\n",
      "    (conv2): Conv2d(original_name=Conv2d)\n",
      "    (bn2): BatchNorm2d(original_name=BatchNorm2d)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "jit_layer1 = jit_model.layer1\n",
    "print(jit_layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "500230b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%self.11 : __torch__.torch.nn.modules.container.Sequential,\n",
       "      %4 : Float(1, 64, 56, 56, strides=[200704, 3136, 56, 1], requires_grad=0, device=cpu)):\n",
       "  %_1.1 : __torch__.torchvision.models.resnet.___torch_mangle_10.BasicBlock = prim::GetAttr[name=\"1\"](%self.11)\n",
       "  %_0.1 : __torch__.torchvision.models.resnet.BasicBlock = prim::GetAttr[name=\"0\"](%self.11)\n",
       "  %6 : Tensor = prim::CallMethod[name=\"forward\"](%_0.1, %4)\n",
       "  %7 : Tensor = prim::CallMethod[name=\"forward\"](%_1.1, %6)\n",
       "  return (%7)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jit_layer1.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ef5e2",
   "metadata": {},
   "source": [
    "## 模型优化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "841fbced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    argument_1: Tensor) -> Tensor:\n",
      "  _1 = getattr(self, \"1\")\n",
      "  _0 = getattr(self, \"0\")\n",
      "  bn2 = _0.bn2\n",
      "  conv2 = _0.conv2\n",
      "  relu = _0.relu\n",
      "  bn1 = _0.bn1\n",
      "  conv1 = _0.conv1\n",
      "  weight = conv1.weight\n",
      "  input = torch._convolution(argument_1, weight, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)\n",
      "  running_var = bn1.running_var\n",
      "  running_mean = bn1.running_mean\n",
      "  bias = bn1.bias\n",
      "  weight0 = bn1.weight\n",
      "  input0 = torch.batch_norm(input, weight0, bias, running_mean, running_var, False, 0.10000000000000001, 1.0000000000000001e-05, True)\n",
      "  input1 = torch.relu_(input0)\n",
      "  weight1 = conv2.weight\n",
      "  input2 = torch._convolution(input1, weight1, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)\n",
      "  running_var0 = bn2.running_var\n",
      "  running_mean0 = bn2.running_mean\n",
      "  bias0 = bn2.bias\n",
      "  weight2 = bn2.weight\n",
      "  out = torch.batch_norm(input2, weight2, bias0, running_mean0, running_var0, False, 0.10000000000000001, 1.0000000000000001e-05, True)\n",
      "  input3 = torch.add_(out, argument_1)\n",
      "  input4 = torch.relu_(input3)\n",
      "  bn20 = _1.bn2\n",
      "  conv20 = _1.conv2\n",
      "  relu0 = _1.relu\n",
      "  bn10 = _1.bn1\n",
      "  conv10 = _1.conv1\n",
      "  weight3 = conv10.weight\n",
      "  input5 = torch._convolution(input4, weight3, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)\n",
      "  running_var1 = bn10.running_var\n",
      "  running_mean1 = bn10.running_mean\n",
      "  bias1 = bn10.bias\n",
      "  weight4 = bn10.weight\n",
      "  input6 = torch.batch_norm(input5, weight4, bias1, running_mean1, running_var1, False, 0.10000000000000001, 1.0000000000000001e-05, True)\n",
      "  input7 = torch.relu_(input6)\n",
      "  weight5 = conv20.weight\n",
      "  input8 = torch._convolution(input7, weight5, None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1, False, False, True, True)\n",
      "  running_var2 = bn20.running_var\n",
      "  running_mean2 = bn20.running_mean\n",
      "  bias2 = bn20.bias\n",
      "  weight6 = bn20.weight\n",
      "  out0 = torch.batch_norm(input8, weight6, bias2, running_mean2, running_var2, False, 0.10000000000000001, 1.0000000000000001e-05, True)\n",
      "  input9 = torch.add_(out0, input4)\n",
      "  return torch.relu_(input9)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n上面代码中我们使用了一个名为inline的pass，将所有子模块进行内联，这样我们就能看见更完整的推理代码。\\npass是一个来源于编译原理的概念，一个 TorchScript 的 pass 会接收一个图，遍历图中所有元素进行某种变换，\\n生成一个新的图。我们这里用到的inline起到的作用就是将模块调用展开，尽管这样做并不能直接影响执行效率，\\n但是它其实是很多其他pass的基础。PyTorch 中定义了非常多的 pass 来解决各种优化任务，\\n未来我们会做一些更详细的介绍。 \\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用inline pass，对graph做变换 \n",
    "torch._C._jit_pass_inline(jit_layer1.graph) \n",
    "print(jit_layer1.code) \n",
    "\n",
    "\"\"\"\n",
    "上面代码中我们使用了一个名为inline的pass，将所有子模块进行内联，这样我们就能看见更完整的推理代码。\n",
    "pass是一个来源于编译原理的概念，一个 TorchScript 的 pass 会接收一个图，遍历图中所有元素进行某种变换，\n",
    "生成一个新的图。我们这里用到的inline起到的作用就是将模块调用展开，尽管这样做并不能直接影响执行效率，\n",
    "但是它其实是很多其他pass的基础。PyTorch 中定义了非常多的 pass 来解决各种优化任务，\n",
    "未来我们会做一些更详细的介绍。 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69feab",
   "metadata": {},
   "source": [
    "## 序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5abdeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型序列化 \n",
    "jit_model.save('jit_model.pth')\n",
    "# 加载序列化后的模型 \n",
    "jit_model = torch.jit.load('jit_model.pth') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe5adb",
   "metadata": {},
   "source": [
    "序列化后的模型不再与 python 相关，可以被部署到各种平台上。\n",
    "\n",
    "PyTorch 提供了可以用于 TorchScript 模型推理的 c++ API，序列化后的模型终于可以不依赖 python 进行推理了： "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8421b734",
   "metadata": {},
   "source": [
    "### 与onnx关系\n",
    "ONNX 是业界广泛使用的一种神经网络中间表示，PyTorch 自然也对 ONNX 提供了支持。\n",
    "torch.onnx.export函数可以帮助我们把 PyTorch 模型转换成 ONNX 模型，\n",
    "这个函数会使用 trace 的方式记录 PyTorch 的推理过程。聪明的同学可能已经想到了，没错，ONNX 的导出，使用的正是 TorchScript 的 trace 工具。具体步骤如下：\n",
    "\n",
    "- 使用 trace 的方式先生成一个 TorchScipt 模型，如果你转换的本身就是 TorchScript 模型，则可以跳过这一步。 \n",
    "- 使用许多 pass 对 1 中生成的模型进行变换，其中对 ONNX 导出最重要的一个 pass 就是ToONNX，这个 pass 会进行一个映射，将 TorchScript 中prim、aten空间下的算子映射到onnx空间下的算子。 \n",
    "- 使用 ONNX 的 proto 格式对模型进行序列化，完成 ONNX 的导出。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b4641",
   "metadata": {},
   "source": [
    "### 与torch.fx关系\n",
    "PyTorch1.9 开始添加了torch.fx工具，根据官方的介绍，它由符号追踪器（symbolic tracer），中间表示（IR）， \n",
    "Python 代码生成（Python code generation）等组件组成，实现了python->python的翻译。是不是和 TorchScript 看起来有点像？ \n",
    "其实他们之间联系不大，可以算是互相垂直的两个工具，为解决两个不同的任务而诞生。\n",
    "- TorchScript 的主要用途是进行模型部署，需要记录生成一个便于推理优化的 IR，对计算图的编辑通常都是面向性能提升等等，不会给模型本身添加新的功能。 \n",
    "- FX 的主要用途是进行python->python的翻译，它的 IR 中节点类型更简单，比如函数调用、属性提取等等，这样的 IR 学习成本更低更容易编辑。使用 FX 来编辑图通常是为了实现某种特定功能，比如给模型插入量化节点等，避免手动编辑网络造成的重复劳动。 \n",
    "\n",
    "这两个工具可以同时使用，比如使用 FX 工具编辑模型来让训练更便利、功能更强大；然后用 TorchScript 将模型加速部署到特定平台。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e531a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
